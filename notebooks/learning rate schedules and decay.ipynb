{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedulers\n",
    "Two different kinds of learning rate schedulers are discussed here: (i) keras default decay and (ii) polynomial decay. \n",
    "\n",
    "As we know that starting with a high learning rate is good to take big jumps in the loss landscape and then, slowly \n",
    "decrease the learning rate so that it converges to the local/global minima. These high LR and min LR can be simply found by exploring various learning rates. It is assumed that you have found out those LRs already. \n",
    "\n",
    "Purpose of this jupyter notebook is to plot the schedulers and to see if they go down to the min LR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters \n",
    "TRAIN_SAMPLES = 2400\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "INIT_LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three sets of learning rate decays which can be used along with SGD optimizer \n",
    "class PolynomialDecay():\n",
    "    \"\"\" polynomial decay of the learning rate. Note: if power = 1 then it is a linear decay \"\"\"\n",
    "    def __init__(self, maxEpochs=EPOCHS, initLR=INIT_LR, power=1.0):\n",
    "        self.maxEpochs = maxEpochs \n",
    "        self.initLR = initLR \n",
    "        self.power = power \n",
    "        self.epochs = []\n",
    "        self.lrs = []\n",
    "        \n",
    "    def __call__(self, epoch):\n",
    "        \"\"\" compute the new learning rate based on polynomial decay \"\"\"\n",
    "        decay = (1 - (epoch / float(self.maxEpochs))) ** self.power \n",
    "        lr = self.initLR * decay\n",
    "        \n",
    "        # save the epochs and lrs for plotting \n",
    "        self.epochs.append(epoch)\n",
    "        self.lrs.append(lr)\n",
    "        \n",
    "        return float(lr)\n",
    "    \n",
    "    def plot(self):        \n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        plt.plot(self.epochs, self.lrs)\n",
    "        plt.title(f\"Polynomial LR Scheduler with {self.power} degree\")\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Learning Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_decay = PolynomialDecay(power=2)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(poly_decay(i))\n",
    "    \n",
    "poly_decay.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three sets of learning rate decays which can be used along with SGD optimizer \n",
    "class KerasDecay():\n",
    "    \"\"\" Keras decaying of the learning rate \"\"\"\n",
    "    def __init__(self, initLR=INIT_LR, decay=None):\n",
    "        self.initLR = initLR\n",
    "        if decay is None: # then use a default decay value \n",
    "            self.decay = self.initLR / BATCH_SIZE\n",
    "            print(f\"using the default decay rate: {self.decay}\")\n",
    "        else:\n",
    "            self.decay = decay \n",
    "        self.iterations = TRAIN_SAMPLES / BATCH_SIZE # it is steps per epoch or, total num of batches per epoch \n",
    "        self.epochs = []\n",
    "        self.lrs = []\n",
    "        \n",
    "    def __call__(self, epoch):\n",
    "        \"\"\" compute the new learning rate based on every batch update \"\"\"\n",
    "        lr = self.initLR * (1. / (1. + self.decay * (epoch *self.iterations))) \n",
    "        \n",
    "        # save the epochs and lrs for plotting \n",
    "        self.epochs.append(epoch)\n",
    "        self.lrs.append(lr)\n",
    "        return float(lr)\n",
    "    \n",
    "    def plot(self):        \n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        plt.plot(self.epochs, self.lrs)\n",
    "        plt.title(f\"Keras default LR Scheduler\")\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Learning Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras_decay = KerasDecay(decay=1e-2)\n",
    "#keras_decay = KerasDecay()\n",
    "keras_decay = KerasDecay(decay = 0.01/EPOCHS)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(keras_decay(i))\n",
    "    \n",
    "keras_decay.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
