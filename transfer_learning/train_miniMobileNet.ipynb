{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D, Activation\n",
    "from tensorflow.keras.layers import SeparableConv2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import LambdaCallback, TensorBoard\n",
    "from tensorflow.keras import backend as K \n",
    "import tempfile \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters \n",
    "LOCAL_COMP = False # either to use local computer or remote GPU PC\n",
    "MODEL_PATH = \"./models/mobilenetv2_.h5\"\n",
    "TRAIN_SAMPLES = 2400\n",
    "VALIDATION_SAMPLES = 490\n",
    "TEST_SAMPLES = 490\n",
    "NUM_CLASSES = 2\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "LABELS = [\"fire\", \"nofire\"]\n",
    "LR_LOSS_PLOT_PATH = \"./models/lr_loss_plot.png\"\n",
    "INIT_LR = 0.01 # this needs to be set as the max LR from the finding LR plot\n",
    "FIND_LR_ENABLE = False # at first set it always to true to find out the best LRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the paths either to a local computer or remote PC (GPU enabled one)\n",
    "if LOCAL_COMP:\n",
    "    TRAIN_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/train/'\n",
    "    VALIDATION_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/val/'\n",
    "    TEST_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/test/'\n",
    "    \n",
    "else:\n",
    "    TRAIN_DATA_DIR = '/home/sanchit/Documents/Projects/datasets/fire_and_smoke_data/train/'\n",
    "    VALIDATION_DATA_DIR = '/home/sanchit/Documents/Projects/datasets/fire_and_smoke_data/val/'\n",
    "    TEST_DATA_DIR = '/home/sanchit/Documents/Projects/datasets/fire_and_smoke_data/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data generators\n",
    "create data generators for both training and validation sets and apply data augmentation only to training generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data generator and initialize it with data augmentation methods \n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, \n",
    "                                   horizontal_flip=True, \n",
    "                                   vertical_flip=True,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   brightness_range=[0.6, 1.3],\n",
    "                                   rotation_range=90, \n",
    "                                   zoom_range=0.15, fill_mode=\"reflect\")\n",
    "\n",
    "# and validation data generator\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=12345,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    VALIDATION_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model \n",
    "we are using transfer learning for training our model, therefore, freeze the main layers of the model and attach a new custom F.C. classifier on the top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model \n",
    "def create_model():\n",
    "    \n",
    "    input_img = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3), name=\"input\")\n",
    "    \n",
    "    x = SeparableConv2D(32, (3,3), strides=1, padding=\"same\")(input_img)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = SeparableConv2D(32, (3,3), strides=1, padding=\"same\")(input_img)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = SeparableConv2D(32, (3,3), strides=(2,2), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = SeparableConv2D(32, (3,3), strides=1, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = SeparableConv2D(64, (3,3), strides=(2,2), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = SeparableConv2D(64, (3,3), strides=1, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "        \n",
    "    x = SeparableConv2D(128, (3,3), strides=(2,2), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = SeparableConv2D(128, (3,3), strides=1, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = SeparableConv2D(256, (3,3), strides=(2,2), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = SeparableConv2D(256, (3,3), strides=1, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = SeparableConv2D(256, (3,3), strides=(2,2), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1, name=\"last_dense\")(x)\n",
    "    output = Activation(\"sigmoid\", name=\"last_softmax\")(x)\n",
    "        \n",
    "    # create the final model \n",
    "    model = Model(inputs = input_img, outputs = output)\n",
    "    model.summary()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model \n",
    "model = create_model()\n",
    "\n",
    "# choose an optimize. Note for finding the best LR, it does not matter what lr is set \n",
    "# here! because during the best LR search, it will going to set its own LRs, however, at the end, \n",
    "# it will set back this original LRs. \n",
    "opt = tf.keras.optimizers.Adam()\n",
    "#opt = tf.keras.optimizers.Adam(lr=INIT_LR)\n",
    "#opt = tf.keras.optimizers.SGD(lr=INIT_LR, decay=INIT_LR/10, momentum=0.9)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best learning rates range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "    \"\"\" Objective is to find a best learning rate by plotting various losses against a list of learning \n",
    "    rates which range from very high to very low. Optimal LR should lie somewhere inside of this range. \n",
    "    \n",
    "    Starting and ending LRs which are chosen are too low (where network is unable to learn, thus a high \n",
    "    loss and too high (where loss is also high), respectively). Therefore, a good range of min and max LR \n",
    "    bounds should be somewhere inside of this range and finding that good range is the objective of this class. \n",
    "    \n",
    "    At the end, entire network can be then trained by using either the correct LR (min LR) or min and max LRs \n",
    "    with a Cyclic LR scheduler. \n",
    "    \n",
    "    Reference: https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/\n",
    "    \"\"\"\n",
    "    def __init__(self, model, stopFactor=4, beta=0.98):\n",
    "        \"\"\" initializes variables for finding the LRs \n",
    "            \n",
    "            :param model: model for which LRs and losses are plotted and analyzed \n",
    "            :param stopFactor: stop factor when the LR becomes too large, then stop the model training automatically \n",
    "            :param beta: used for averaging the loss value \n",
    "            :param lrs: a list of tried LR values \n",
    "            :param losses: a list of tried loss values \n",
    "            :param avgLoss: average loss value over time \n",
    "            :param batchNum: current batch number \n",
    "            :param bestLoss: best loss (of course, lowest) found so far during training \n",
    "            :param lrMult: LR multiplication factor \n",
    "            :weightsFile: filename to save initial (original) weights of the model \n",
    "        \"\"\"\n",
    "        \n",
    "        # store the model, stop factor, and beta value (for computing\n",
    "        # a smoothed, average loss)\n",
    "        self.model = model\n",
    "        self.stopFactor = stopFactor\n",
    "        self.beta = beta\n",
    "\n",
    "        # initialize our list of learning rates and losses,\n",
    "        # respectively\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "\n",
    "        # initialize our learning rate multiplier, average loss, best\n",
    "        # loss found thus far, current batch number, and weights file\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "\n",
    "    def reset(self):\n",
    "        # re-initialize all variables from our constructor\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "\n",
    "    def is_data_iter(self, data):\n",
    "        # define the set of class types we will check for\n",
    "        iterClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\",\n",
    "             \"DataFrameIterator\", \"Iterator\", \"Sequence\"]\n",
    "\n",
    "        # return whether our data is an iterator\n",
    "        return data.__class__.__name__ in iterClasses\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        \"\"\" following are the steps/things which is happening in this function:\n",
    "            - this function runs after every batch update \n",
    "            - recording of current loss values which is smoothen up first \n",
    "            - recording of best loss and updating of it if a new one has been found \n",
    "            - checking if the loss has grown too much, then stop the model training \n",
    "            - setting up of a new LR for the next model training with multiplying current \n",
    "              LR with the LR-multiplier at every batch update\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # get the current LR from the model and save to a list of LRs which have been used already \n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # grab the loss at the end of this batch, increment the total\n",
    "        # number of batches processed, compute the average average\n",
    "        # loss, smooth it, and update the losses list with the\n",
    "        # smoothed value\n",
    "        l = logs[\"loss\"] # NOTE: it contains the current loss of the training\n",
    "        self.batchNum += 1\n",
    "        self.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
    "        smooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
    "        self.losses.append(smooth)\n",
    "\n",
    "        # compute the maximum loss stopping factor value\n",
    "        stopLoss = self.stopFactor * self.bestLoss\n",
    "\n",
    "        # check to see whether the loss has grown too large\n",
    "        if self.batchNum > 1 and smooth > stopLoss:\n",
    "            # stop returning and return from the method\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        # check to see if the best loss should be updated\n",
    "        if self.batchNum == 1 or smooth < self.bestLoss:\n",
    "            self.bestLoss = smooth\n",
    "\n",
    "        # increase the learning rate ans set it as a new LR for the model training \n",
    "        lr *= self.lrMult # self.lrMult is found in the find() method\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def find(self, trainData, startLR, endLR, epochs=None,\n",
    "        stepsPerEpoch=None, batchSize=32, sampleSize=2048,\n",
    "        classWeight=None, verbose=1):\n",
    "        \"\"\" following are the steps/things which is happening in this function:\n",
    "            - only using the training data for computing the loss, i.e., no split of it into test/val data \n",
    "            - loss log are transferred via LambdaCallback function to \"on_batch_end()\" fn. \n",
    "            - LR multiplier is computed once. And, it is a fixed (uniform) interval computed from endLR, startLR \n",
    "                over total numbers of batches \n",
    "            - Model's original weights and LR are temporarily saved and they get imported back again after \n",
    "              the plot for finding the best LR is done. \n",
    "        \"\"\"\n",
    "        \n",
    "        # reset our class-specific variables\n",
    "        self.reset()\n",
    "\n",
    "        # determine if we are using a data generator or not\n",
    "        useGen = self.is_data_iter(trainData)\n",
    "\n",
    "        # if we're using a generator and the steps per epoch is not\n",
    "        # supplied, raise an error\n",
    "        if useGen and stepsPerEpoch is None:\n",
    "            msg = \"Using generator without supplying stepsPerEpoch\"\n",
    "            raise Exception(msg)\n",
    "\n",
    "        # if we're not using a generator then our entire dataset must\n",
    "        # already be in memory\n",
    "        elif not useGen:\n",
    "            # grab the number of samples in the training data and\n",
    "            # then derive the number of steps per epoch\n",
    "            numSamples = len(trainData[0])\n",
    "            stepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
    "\n",
    "        # if no number of training epochs are supplied, compute the\n",
    "        # training epochs based on a default sample size\n",
    "        if epochs is None:\n",
    "            epochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
    "\n",
    "        # compute the total number of batch updates that will take\n",
    "        # place while we are attempting to find a good starting\n",
    "        # learning rate\n",
    "        numBatchUpdates = epochs * stepsPerEpoch\n",
    "\n",
    "        # derive the learning rate multiplier based on the ending\n",
    "        # learning rate, starting learning rate, and total number of\n",
    "        # batch updates\n",
    "        self.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
    "\n",
    "        # save the model's original weights, so we can reset the weights when we are \n",
    "        # done finiding the optimal learning rates \n",
    "        self.weightsFile = tempfile.mkstemp()[1]\n",
    "        self.model.save_weights(self.weightsFile)\n",
    "\n",
    "        # grab the *original* learning rate (so we can reset it\n",
    "        # later), and then set the *starting* learning rate\n",
    "        origLR = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, startLR)\n",
    "\n",
    "        # construct a callback that will be called at the end of each\n",
    "        # batch, enabling us to increase our learning rate as training\n",
    "        # progresses\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs:\n",
    "            self.on_batch_end(batch, logs))\n",
    "\n",
    "        # check to see if we are using a data iterator\n",
    "        if useGen:\n",
    "            self.model.fit_generator(\n",
    "                trainData,\n",
    "                steps_per_epoch=stepsPerEpoch,\n",
    "                epochs=epochs,\n",
    "                class_weight=classWeight,\n",
    "                verbose=verbose,\n",
    "                callbacks=[callback])\n",
    "\n",
    "        # otherwise, our entire training data is already in memory\n",
    "        else:\n",
    "            # train our model using Keras' fit method\n",
    "            self.model.fit(\n",
    "                trainData[0], trainData[1],\n",
    "                batch_size=batchSize,\n",
    "                epochs=epochs,\n",
    "                class_weight=classWeight,\n",
    "                callbacks=[callback],\n",
    "                verbose=verbose)\n",
    "\n",
    "        # finally, when we are done, set back the original model's weights and LR values \n",
    "        self.model.load_weights(self.weightsFile)\n",
    "        K.set_value(self.model.optimizer.lr, origLR)\n",
    "\n",
    "    def plot_loss(self, skipBegin=10, skipEnd=1, title=\"\"):\n",
    "        # grab the learning rate and losses values to plot\n",
    "        lrs = self.lrs[skipBegin:-skipEnd]\n",
    "        losses = self.losses[skipBegin:-skipEnd]\n",
    "\n",
    "        # plot the learning rate vs. loss\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning Rate (Log Scale)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "\n",
    "        # if the title is not empty, add it to the plot\n",
    "        if title != \"\":\n",
    "            plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best learning rates range only at the first run of this script. \n",
    "if FIND_LR_ENABLE:\n",
    "    lrf = LearningRateFinder(model)\n",
    "    # increasing learning rates at a regular interval from a very low to very high range. Then, compute \n",
    "    # the training loss. Objective here is to find a range of LRs where the loss varies the most. That means in those \n",
    "    # LRs model is the learning the most. \n",
    "    lrf.find(train_generator, \n",
    "            startLR=1e-10, \n",
    "            endLR=1e+1, \n",
    "            epochs=20,\n",
    "            stepsPerEpoch=TRAIN_SAMPLES // BATCH_SIZE, \n",
    "            batchSize=BATCH_SIZE)\n",
    "\n",
    "    # plot the loss for the various learning rates and save the\n",
    "    # resulting plot to disk\n",
    "    lrf.plot_loss()\n",
    "    plt.savefig(LR_LOSS_PLOT_PATH)\n",
    "\n",
    "    # exit the script so we can adjust our learning rates\n",
    "    # in the config and then train the network for our full set of\n",
    "    # epochs\n",
    "    print(\"[INFO] learning rate finder complete\")\n",
    "    print(\"[INFO] examine plot and adjust learning rates before training\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Train model \n",
    "After finding the min and max range of the best possible learning rates, train the whole model with the following \n",
    "possibilities: \n",
    "* If ADAM is used, then set the max of the range as the starting LR. As it will adapt it gradually towards a lower value. \n",
    "  \n",
    "  \n",
    "* If SGD is used, then max of the range as the starting LR and use either default decay or polynomial decay. But be careful that decay should not go beyond the min of the range. In other words, decay should gradually reach to the min of the range. \n",
    "\n",
    "\n",
    "* If SGD with Cyclic LR is used, then use [min_lr, max_lr] as the range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define callbacks before starting the training\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\", verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=3, min_lr=0.00001, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(MODEL_PATH, monitor=\"val_acc\", save_best_only=True, mode='max', verbose=1)\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard = TensorBoard(log_dir=logdir, histogram_freq=1)\n",
    "#callbacks = [reduce_lr, early_stop, model_checkpoint, tensorboard]\n",
    "callbacks = [early_stop, model_checkpoint, tensorboard]\n",
    "\n",
    "# perform model training \n",
    "H = model.fit_generator(generator=train_generator, \n",
    "                    steps_per_epoch=TRAIN_SAMPLES // BATCH_SIZE, \n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=VALIDATION_SAMPLES // BATCH_SIZE, \n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# launch TensorBoard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results \n",
    "Plot Confusion matrix and classification report on both validation and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mismatches(y_true: list, y_pred: list, filenames: list):\n",
    "    \"\"\" return a list of wrong predictions (or, mis-matches) done by the trained model \n",
    "        :param y_true: a list of true (or ground-truth) labels \n",
    "        :param y_pred: a list of predicted labels \n",
    "        :param filenames: a list of filenames corresponding to the true labels\n",
    "    \"\"\"\n",
    "    mismatches_imgs = list(dict())\n",
    "    for i in range(0, len(y_true)):\n",
    "        if y_true[i] != y_pred[i]:\n",
    "            print(f\"True class: {LABELS[y_true[i]]}({y_true[i]}) and predicted as: {LABELS[y_pred[i]]}({y_pred[i]})\")\n",
    "            temp = dict()\n",
    "            temp[\"filename\"] = filenames[i]\n",
    "            temp[\"true label\"] = y_true[i]\n",
    "            temp[\"pred label\"] = y_pred[i]\n",
    "            mismatches_imgs.append(temp)\n",
    "        continue\n",
    "    return mismatches_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix, classification report and mis-matches information for the validation data\n",
    "y_pred = model.predict_generator(validation_generator, steps=VALIDATION_SAMPLES // BATCH_SIZE + 1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true=validation_generator.classes, y_pred=y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true=validation_generator.classes, y_pred=y_pred, target_names=LABELS))\n",
    "print(\"Mismatches information:\")\n",
    "print(get_mismatches(validation_generator.classes, y_pred, validation_generator.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, plot confusion for the test dataset\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix, classification report and mis-classification (or, mis-matches) information for the test data\n",
    "y_pred = model.predict_generator(test_generator, steps=TEST_SAMPLES // BATCH_SIZE + 1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true=test_generator.classes, y_pred=y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true=test_generator.classes, y_pred=y_pred, target_names=LABELS))\n",
    "print(\"Mismatches information:\")\n",
    "print(get_mismatches(test_generator.classes, y_pred, validation_generator.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
