{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "F6mFisPi4Fxt",
    "outputId": "cb96b117-a870-4b95-c126-5e8aa4d91ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL4aEv4Y4Fxx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, AveragePooling2D, SeparableConv2D\n",
    "from keras.layers import Activation, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LambdaCallback \n",
    "from keras import backend as K \n",
    "import tempfile \n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_9XL0_y4Fx4"
   },
   "source": [
    "### Create all the config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icBpMUQO4Fx4"
   },
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224 \n",
    "\n",
    "train_dir = \"/content/drive/My Drive/Datasets/image_quality/training/\"\n",
    "\n",
    "valid_dir = \"/content/drive/My Drive/Datasets/image_quality/evaluation/\"\n",
    "\n",
    "trained_model_path = \"/content/drive/My Drive/Datasets/image_quality/model.h5\"\n",
    "\n",
    "training_plot_path = \"/content/drive/My Drive/Datasets/image_quality/model_training_plot.png\"\n",
    "\n",
    "lr_loss_plot_path = \"/content/drive/My Drive/Datasets/image_quality/lr_loss_plot.png\"\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "num_train_samples = 1290\n",
    "\n",
    "num_test_samples = 207 \n",
    "\n",
    "epochs = 50\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "input_shape = (img_width, img_height, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qc-eUi3x4Fx7"
   },
   "source": [
    "### Create CNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OL7_XpZo4eqs",
    "outputId": "089c4b2d-d2e3-4317-c6eb-f5c4578a6930"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.path.exists(\"/content/drive/My Drive/Datasets/image_quality/evaluation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_b6teAXj4Fx8"
   },
   "outputs": [],
   "source": [
    "# define a class which creates a CNN model \n",
    "class ConvNetModel:\n",
    "  \"\"\"Simple sequential Conv-net model\"\"\"\n",
    "  def __init__(self, num_classes, input_shape):\n",
    "      self.num_classes = num_classes \n",
    "      self.input_shape = input_shape\n",
    "\n",
    "  @staticmethod\n",
    "  def conv_block(x, num_filters, kernels=(3,3), strides=(1, 1), is_norm_conv=True):\n",
    "      if is_norm_conv:\n",
    "          x = Conv2D(num_filters, \n",
    "                     kernels,\n",
    "                     strides=strides,\n",
    "                     padding=\"same\", \n",
    "                     kernel_initializer=\"he_normal\", \n",
    "                     kernel_regularizer=l2(0.002))(x)\n",
    "      else:\n",
    "          x = SeparableConv2D(num_filters,\n",
    "                              kernels,\n",
    "                              strides=strides,\n",
    "                              padding=\"same\", \n",
    "                              depthwise_initializer=\"he_normal\", \n",
    "                              pointwise_initializer=\"he_normal\",\n",
    "                              depthwise_regularizer=l2(0.002), \n",
    "                              pointwise_regularizer=l2(0.002))(x)\n",
    "\n",
    "      x = BatchNormalization()(x)\n",
    "      x = Activation('relu')(x)\n",
    "      return x\n",
    "\n",
    "  def build(self):\n",
    "      input_img = Input(shape = self.input_shape, name=\"input_image\")\n",
    "      x = ConvNetModel.conv_block(input_img, 32, kernels=(3,3), strides=(2, 2), is_norm_conv=False)\n",
    "      x = Dropout(0.5)(x)\n",
    "      x = ConvNetModel.conv_block(x, 64, kernels=(3,3), strides=(2, 2), is_norm_conv=False)\n",
    "      x = Dropout(0.5)(x)\n",
    "      x = ConvNetModel.conv_block(x, 128, kernels=(3,3), strides=(2, 2), is_norm_conv=False)\n",
    "      x = Dropout(0.5)(x)\n",
    "      x = ConvNetModel.conv_block(x, 256, kernels=(3,3), strides=(2, 2), is_norm_conv=False)\n",
    "      x = Dropout(0.5)(x)\n",
    "      x = ConvNetModel.conv_block(x, 512, kernels=(3,3), strides=(2, 2), is_norm_conv=False)\n",
    "      x = Dropout(0.5)(x)\n",
    "      x = AveragePooling2D(7, 7)(x)\n",
    "      x = Flatten()(x)\n",
    "      x = Dense(self.num_classes)(x)\n",
    "      output = Activation(\"softmax\")(x)\n",
    "      model = Model(inputs=input_img, outputs=output)\n",
    "      model.summary()\n",
    "\n",
    "      return model       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfsJWpr54FyB"
   },
   "source": [
    "### Create, build and compile ConvNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EUo73CbV4FyD",
    "outputId": "faa29bd5-37d1-43e6-d908-9442fbe989a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_11 (Separab (None, 112, 112, 32)      155       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_12 (Separab (None, 56, 56, 64)        2400      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_13 (Separab (None, 28, 28, 128)       8896      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_14 (Separab (None, 14, 14, 256)       34176     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_15 (Separab (None, 7, 7, 512)         133888    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 184,509\n",
      "Trainable params: 182,525\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create and compile a model     \n",
    "cnn_model = ConvNetModel(num_classes, input_shape)\n",
    "model = cnn_model.build()\n",
    "\n",
    "# initialze an optimizer\n",
    "#opt = optimizers.Adam(decay=1e-6, amsgrad=False)\n",
    "opt = optimizers.SGD(momentum=0.9)\n",
    "\n",
    "# compile it with an optimizer\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer= opt, \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VU51ZnusV3Ub"
   },
   "source": [
    "### **Create data (augmentation) generators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "39TuhItEdhAA",
    "outputId": "99754910-fd9a-48e8-e831-bb760e71e162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1290 images belonging to 2 classes.\n",
      "Found 217 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# generate augmented training data \n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   horizontal_flip=True, \n",
    "                                   vertical_flip=True,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   zoom_range=0.15, fill_mode=\"reflect\")\n",
    "\n",
    "# this is the augmentation configuration we will use for testing: only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ6YdTiH9aOP"
   },
   "source": [
    "### **Find Learning Rates (min and max bounds)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZK01klm9_d7"
   },
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "  \"\"\" goal of this class is to provide a plot where effects of using a range \n",
    "  of learning rates on the loss is displayed. Some LRs are too low and some are \n",
    "  too high, therefore, with the help of this plot, we can find an optimal range of \n",
    "  LRs (minimum and maximum bounds). \n",
    "\n",
    "  Starting and ending LRs choosen for the plot are too low (where network is unable \n",
    "  to learn) or too large (where loss is too high). Therefore, a good range of min and \n",
    "  max LR bounds should be somewhere inside of it and that's the aim of this class. \n",
    "\n",
    "  A careful analysis of the plot is required to find the right bounds. At the end, \n",
    "  entire network will be then trained by using correct learning rate (max learning rate) or min and max LRs. \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, model, stopFactor=4, beta=0.98):\n",
    "      \"\"\" initializes variables for finding the learning rates \n",
    "\n",
    "          :param model: model for which learning rates and losses will be analyzed\n",
    "          :param stopFactor: stop factor when the learning rate becomes too large to stop the model training process\n",
    "          :param beta: used for averaging the loss value\n",
    "          :param lrs: a list of tried LR values\n",
    "          :param losses: a list of tried loss values\n",
    "          :param avgLoss: average loss value over time \n",
    "          :param batchNum: current batch number \n",
    "          :param bestLoss: best low (of course lowest) found so far during training\n",
    "          :param lrMult: learning rate multiplication factor \n",
    "          :weightsFile: filename to save initial (original) weights of the model\n",
    "      \"\"\"\n",
    "      self.model = model\n",
    "      self.stopFactor = stopFactor # not clear why 4? \n",
    "\n",
    "      self.beta = beta \n",
    "      self.lrs = [] # list of LRs which have been used already\n",
    "      self.losses = [] # list of losses so far in the batch updates\n",
    "      self.avgLoss = 0\n",
    "      self.batchNum = 0 # current batch number/index\n",
    "\n",
    "      self.bestLoss = 1e9 \n",
    "      self.lrMult = 1\n",
    "      self.weightsFile = None\n",
    "\n",
    "  def reset(self):\n",
    "      \"\"\" reset or re-initialize all variables from our constructor \"\"\"\n",
    "      self.lrs = []\n",
    "      self.losses = []\n",
    "      self.lrMult = 1\n",
    "      self.avgLoss = 0 \n",
    "      self.bestLoss = 1e9 \n",
    "      self.batchNum = 0 \n",
    "      self.weightsFile = None\n",
    "\n",
    "  def is_data_iter(self, data):\n",
    "      # define the set of class types we will check for\n",
    "      iterClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\", \n",
    "                     \"DataFrameIterator\", \"Iterator\", \"Sequence\"]\n",
    "      # return whether our data is an iterator\n",
    "      return data.__class__.__name__ in iterClasses\n",
    "\n",
    "  def on_batch_end(self, batch, logs):\n",
    "      \"\"\" responsible for setting/updating the new learning rate (LR) based on the current LR \n",
    "          and also, for recording current loss and LR. \n",
    "      \"\"\"\n",
    "\n",
    "      # get the current LR from the model and save to a list of LRs which have been used already \n",
    "      lr = K.get_value(self.model.optimizer.lr)\n",
    "      self.lrs.append(lr)\n",
    "\n",
    "      # update the batch number \n",
    "      self.batchNum += 1\n",
    "\n",
    "      # get the loss at the end of this batch, compute the average loss, smooth it and save to a list \n",
    "      # of losses \n",
    "      l = logs[\"loss\"]\n",
    "      self.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta)*l)\n",
    "      smooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
    "      self.losses.append(smooth)\n",
    "\n",
    "      # compute maximum loss as a factor of best loss to stop the training \n",
    "      stopLoss = self.stopFactor * self.bestLoss \n",
    "\n",
    "      # check whether the loss has grown too large, therefore, stop the training \n",
    "      if self.batchNum > 1 and smooth > stopLoss: \n",
    "          self.model.stop_training = True \n",
    "          return \n",
    "\n",
    "      # check if the best loss should be updated \n",
    "      if self.batchNum == 1 or smooth < self.bestLoss:\n",
    "          self.bestLoss = smooth \n",
    "\n",
    "      # finally increase the LR and set it as new LR for model training \n",
    "      lr *= self.lrMult \n",
    "      K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "  def find(self, trainData, startLR, endLR, epochs=None, \n",
    "           stepsPerEpoch=None, batchSize=32, sampleSize=2048, \n",
    "           verbose=1):\n",
    "      \"\"\" finds different learning rates (including optimal one) via model training \n",
    "          and at each batch update, new LR is update and saves current LR and loss. \n",
    "\n",
    "          :param trainData: training data either Numpy array data or data generator \n",
    "          :param startLR: starting learning rate \n",
    "          :param endLR: last learning rate \n",
    "          :param epochs: number of epochs to train for (if not value provided, it is calculate on a default sampleSize)\n",
    "          :param stepsPerEpoch: total number of batch update steps per epoch \n",
    "          :sampleSize: size of training data to use when finding the optimal learning rate \n",
    "      \"\"\"\n",
    "\n",
    "      # reset class specific variables \n",
    "      self.reset()\n",
    "\n",
    "      # determine if we are using a data generator or not \n",
    "      useGen = self.is_data_iter(trainData)\n",
    "\n",
    "      # if we are using a generator and the steps_per_epoch is not supplied, \n",
    "      # raise an error \n",
    "      if useGen and stepsPerEpoch is None:\n",
    "          raise Exception (\"Using generator without supplying steps_per_epoch\")\n",
    "\n",
    "      # if we are not using a generator then our entire dataset must already be \n",
    "      # in memory \n",
    "      elif not useGen:\n",
    "          # get number of samples in the training data and then derive the number \n",
    "          # of steps_per_epoch \n",
    "          numSamples = len(trainData[0])\n",
    "          stepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
    "\n",
    "      # if number of epochs is not provided, then compute it based on a \n",
    "      # default sample size \n",
    "      if epochs is None:\n",
    "          epochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
    "\n",
    "      # calculate total number of batch updates that will take place \n",
    "      numBatchUpdates = epochs * stepsPerEpoch \n",
    "\n",
    "      # derive the LR multiplier based on ending LR, starting LR and total \n",
    "      # number of batch updates (exponentially increase LR)\n",
    "      self.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
    "\n",
    "      # save the model's original weights, so we can reset the weights when we are \n",
    "      # done finiding the optimal learning rates \n",
    "      self.weightsFile = tempfile.mkstemp()[1] \n",
    "      self.model.save_weights(self.weightsFile)\n",
    "\n",
    "      # save the model's original LR and then set the new \"starting\" learning rate \n",
    "      origLR = K.get_value(self.model.optimizer.lr)\n",
    "      K.set_value(self.model.optimizer.lr, startLR)\n",
    "\n",
    "      # perform model training and at each batch: update LR and record current LR and loss \n",
    "      # create a callback that will be called at the end of each batch, which increases the LR and \n",
    "      # save current LR and loss\n",
    "      callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "      # check if we are using a data iterator \n",
    "      if useGen:\n",
    "          self.model.fit_generator(\n",
    "                  trainData,\n",
    "                  steps_per_epoch=stepsPerEpoch,\n",
    "                  epochs=epochs,\n",
    "                  verbose=verbose,\n",
    "                  callbacks=[callback])\n",
    "\n",
    "      # otherwise, our entire training data is already in memory \n",
    "      else:\n",
    "          self.model.fit(\n",
    "                  trainData[0], trainData[1],\n",
    "                  batch_size=batchSize,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=[callback],\n",
    "                  verbose=verbose)\n",
    "\n",
    "      # finally, when we are done, set back the original model's weights and LR values \n",
    "      self.model.load_weights(self.weightsFile)\n",
    "      K.set_value(self.model.optimizer.lr, origLR)\n",
    "\n",
    "  def plot_loss(self, skipBegin=10, skipEnd=1, title=\"learning rate finder\"):\n",
    "      \"\"\" plot learning rates versus losses diagram \"\"\"\n",
    "      lrs = self.lrs[skipBegin:-skipEnd]\n",
    "      losses = self.losses[skipBegin:-skipEnd]\n",
    "\n",
    "      # plot the learning rate versus loss\n",
    "      plt.plot(lrs, losses)\n",
    "      plt.xscale(\"log\")\n",
    "      plt.xlabel(\"Learning Rate\")\n",
    "      plt.ylabel(\"Loss\")\n",
    "      plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "XT7RrDRM-PtN",
    "outputId": "b7a1abcf-2673-49a9-c148-7a6c1afc34f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 582s 15s/step - loss: 4.9070 - acc: 0.5103\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 145s 4s/step - loss: 4.8896 - acc: 0.5168\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 132s 3s/step - loss: 4.9008 - acc: 0.5164\n",
      "Epoch 4/10\n",
      "28/40 [====================>.........] - ETA: 39s - loss: 4.8944 - acc: 0.5121"
     ]
    }
   ],
   "source": [
    "lrf = LearningRateFinder(model)\n",
    "lrf.find(\n",
    "    train_generator, \n",
    "    startLR=1e-10, \n",
    "    endLR=1e+1, \n",
    "    epochs=10,\n",
    "    stepsPerEpoch=num_train_samples // batch_size, \n",
    "    batchSize=batch_size)\n",
    "\n",
    "# plot the loss for the various learning rates and save the\n",
    "# resulting plot to disk\n",
    "lrf.plot_loss()\n",
    "plt.savefig(lr_loss_plot_path)\n",
    "\n",
    "# gracefully exit the script so we can adjust our learning rates\n",
    "# in the config and then train the network for our full set of\n",
    "# epochs\n",
    "print(\"[INFO] learning rate finder complete\")\n",
    "print(\"[INFO] examine plot and adjust learning rates before training\")\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nu84zCYC3isI"
   },
   "source": [
    "### **Define a learning rate scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly4pvooz3h24"
   },
   "outputs": [],
   "source": [
    "class PolynomialDecay():\n",
    "\tdef __init__(self, maxEpochs=50, initAlpha=0.01, power=1.0):\n",
    "\t\t# store the maximum number of epochs, base learning rate,\n",
    "\t\t# and power of the polynomial\n",
    "\t\tself.maxEpochs = maxEpochs\n",
    "\t\tself.initAlpha = initAlpha\n",
    "\t\tself.power = power\n",
    " \n",
    "\tdef __call__(self, epoch):\n",
    "\t\t# compute the new learning rate based on polynomial decay\n",
    "\t\tdecay = (1 - (epoch / float(self.maxEpochs))) ** self.power\n",
    "\t\talpha = self.initAlpha * decay\n",
    " \n",
    "\t\t# return the new learning rate\n",
    "\t\treturn float(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EY6oBDjuWjjT"
   },
   "source": [
    "### **Define all the necessary callbacks and train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "V1Man5oDmkPB",
    "outputId": "a225ca22-89f1-4cd8-9876-2f846ad11aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 [==============================] - 126s 3s/step - loss: 4.1560 - acc: 0.5698 - val_loss: 3.6153 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.54688, saving model to /content/drive/My Drive/Datasets/image_quality/model.h5\n",
      "Epoch 2/50\n",
      "40/40 [==============================] - 126s 3s/step - loss: 3.2462 - acc: 0.5775 - val_loss: 2.9006 - val_acc: 0.5568\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.54688 to 0.55676, saving model to /content/drive/My Drive/Datasets/image_quality/model.h5\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 126s 3s/step - loss: 2.6472 - acc: 0.5923 - val_loss: 2.5706 - val_acc: 0.5568\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.55676 to 0.55676, saving model to /content/drive/My Drive/Datasets/image_quality/model.h5\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 122s 3s/step - loss: 2.2435 - acc: 0.6079 - val_loss: 2.0982 - val_acc: 0.5838\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.55676 to 0.58378, saving model to /content/drive/My Drive/Datasets/image_quality/model.h5\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 121s 3s/step - loss: 1.9511 - acc: 0.6508 - val_loss: 1.8015 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.58378 to 0.65946, saving model to /content/drive/My Drive/Datasets/image_quality/model.h5\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 114s 3s/step - loss: 1.7297 - acc: 0.6575 - val_loss: 1.5770 - val_acc: 0.6865\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.65946 to 0.68649, saving model to /content/drive/My Drive/Datasets/image_quality/model.h5\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 121s 3s/step - loss: 1.5795 - acc: 0.6672 - val_loss: 1.4152 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.68649 to 0.72432, saving model to /content/drive/My Drive/Datasets/image_quality/model.h5\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 123s 3s/step - loss: 1.4435 - acc: 0.6621 - val_loss: 1.2868 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.72432 to 0.78125, saving model to /content/drive/My Drive/Datasets/image_quality/model.h5\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 132s 3s/step - loss: 1.3539 - acc: 0.6776 - val_loss: 1.2018 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.78125\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 121s 3s/step - loss: 1.2699 - acc: 0.6728 - val_loss: 1.1690 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.78125\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 119s 3s/step - loss: 1.1784 - acc: 0.6894 - val_loss: 1.0828 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.78125\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 119s 3s/step - loss: 1.1181 - acc: 0.6953 - val_loss: 1.0035 - val_acc: 0.7189\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.78125\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 121s 3s/step - loss: 1.0680 - acc: 0.6995 - val_loss: 0.9314 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.78125 to 0.79459, saving model to /content/drive/My Drive/Datasets/image_quality/model.h5\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 116s 3s/step - loss: 1.0217 - acc: 0.7100 - val_loss: 1.5312 - val_acc: 0.6270\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79459\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 125s 3s/step - loss: 0.9420 - acc: 0.7344 - val_loss: 1.3223 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79459\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 131s 3s/step - loss: 0.8955 - acc: 0.7375 - val_loss: 0.9845 - val_acc: 0.6865\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79459\n",
      "Epoch 17/50\n",
      "40/40 [==============================] - 117s 3s/step - loss: 0.8688 - acc: 0.7415 - val_loss: 1.1923 - val_acc: 0.6378\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.79459\n",
      "Epoch 18/50\n",
      "40/40 [==============================] - 120s 3s/step - loss: 0.8209 - acc: 0.7609 - val_loss: 2.5591 - val_acc: 0.6054\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.79459\n",
      "Epoch 19/50\n",
      "40/40 [==============================] - 119s 3s/step - loss: 0.7663 - acc: 0.7789 - val_loss: 1.4945 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.79459\n",
      "Epoch 20/50\n",
      "40/40 [==============================] - 121s 3s/step - loss: 0.7859 - acc: 0.7703 - val_loss: 1.1319 - val_acc: 0.6973\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79459\n",
      "Epoch 21/50\n",
      "40/40 [==============================] - 117s 3s/step - loss: 0.7664 - acc: 0.7764 - val_loss: 1.1409 - val_acc: 0.6973\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.79459\n",
      "Epoch 22/50\n",
      " 9/40 [=====>........................] - ETA: 2:11 - loss: 0.7705 - acc: 0.7951"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-09e1b54de683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_test_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define callbacks before starting the training\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\", verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(trained_model_path, monitor=\"val_acc\", save_best_only=True, mode='max', verbose=1)\n",
    "#schedule = PolynomialDecay(maxEpochs=epochs, initAlpha=1e-1, power=5)\n",
    "\n",
    "#callbacks = [reduce_lr, early_stop, model_checkpoint, LearningRateScheduler(schedule)]\n",
    "callbacks = [reduce_lr, early_stop, model_checkpoint]\n",
    "\n",
    "# perform model training \n",
    "H = model.fit_generator(generator=train_generator, \n",
    "                    steps_per_epoch=num_train_samples // batch_size, \n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=num_test_samples // batch_size, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7TAHiX72YFA"
   },
   "source": [
    "## **Plot the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "J3BlY1cRmx_q",
    "outputId": "9c5c5549-9980-4b31-8908-d27290c1db5f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-05fe033dac3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ggplot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_acc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, epochs)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(training_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7RM_G-f3R4U"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Simple CNN model Image Quality.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
