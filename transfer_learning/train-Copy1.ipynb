{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import ktrain \n",
    "from ktrain import vision as comp_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/'\n",
    "\n",
    "# configuration parameters \n",
    "TRAIN_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/train/'\n",
    "VALIDATION_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/val/'\n",
    "TEST_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/test/'\n",
    "MODEL_PATH = \"./models/mobilenetv2_.h5\"\n",
    "TRAIN_SAMPLES = 1600\n",
    "VALIDATION_SAMPLES = 430\n",
    "TEST_SAMPLES = 430\n",
    "NUM_CLASSES = 2\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LABELS = [\"fire\", \"nofire\"]\n",
    "LR_LOSS_PLOT_PATH = \"./models/lr_loss_plot.png\"\n",
    "#INIT_LR = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data generators\n",
    "create data generators for both training and validation sets and apply data augmentation only to training generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = comp_vis.get_data_aug(horizontal_flip=True, \n",
    "                                 vertical_flip=True, \n",
    "                                 width_shift_range=0.1, \n",
    "                                 height_shift_range=0.1, \n",
    "                                 brightness_range=[0.6, 1.3], \n",
    "                                 zoom_range=0.15, fill_mode=\"reflect\")\n",
    "\n",
    "(train_data, val_data, preproc) = comp_vis.images_from_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "    \"\"\" goal of this class is to provide a plot where effects of using a range \n",
    "    of learning rates on the loss is displayed. Some LRs are too low and some are \n",
    "    too high, therefore, with the help of this plot, we can find an optimal range of \n",
    "    LRs (minimum and maximum bounds). \n",
    "\n",
    "    Starting and ending LRs choosen for the plot are too low (where network is unable \n",
    "    to learn) or too large (where loss is too high). Therefore, a good range of min and \n",
    "    max LR bounds should be somewhere inside of it and that's the aim of this class. \n",
    "\n",
    "    A careful analysis of the plot is required to find the right bounds. At the end, \n",
    "    entire network will be then trained by using correct learning rate (max learning rate) or min and max LRs.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, stopFactor=4, beta=0.98):\n",
    "        \"\"\" initializes variables for finding the learning rates \n",
    "\n",
    "          :param model: model for which learning rates and losses will be analyzed\n",
    "          :param stopFactor: stop factor when the learning rate becomes too large to stop the model training process\n",
    "          :param beta: used for averaging the loss value\n",
    "          :param lrs: a list of tried LR values\n",
    "          :param losses: a list of tried loss values\n",
    "          :param avgLoss: average loss value over time \n",
    "          :param batchNum: current batch number \n",
    "          :param bestLoss: best low (of course lowest) found so far during training\n",
    "          :param lrMult: learning rate multiplication factor \n",
    "          :weightsFile: filename to save initial (original) weights of the model \"\"\"\n",
    "        \n",
    "        self.model = model\n",
    "        self.stopFactor = stopFactor # not clear why 4? \n",
    "\n",
    "        self.beta = beta \n",
    "        self.lrs = [] # list of LRs which have been used already\n",
    "        self.losses = [] # list of losses so far in the batch updates\n",
    "        self.avgLoss = 0\n",
    "        self.batchNum = 0 # current batch number/index\n",
    "\n",
    "        self.bestLoss = 1e9 \n",
    "        self.lrMult = 1\n",
    "        self.weightsFile = None\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" reset or re-initialize all variables from our constructor \"\"\"\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0 \n",
    "        self.bestLoss = 1e9 \n",
    "        self.batchNum = 0 \n",
    "        self.weightsFile = None\n",
    "        \n",
    "    def is_data_iter(self, data):\n",
    "        # define the set of class types we will check for\n",
    "        iterClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\", \n",
    "                     \"DataFrameIterator\", \"Iterator\", \"Sequence\"]\n",
    "        # return whether our data is an iterator\n",
    "        return data.__class__.__name__ in iterClasses\n",
    "    \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        \"\"\" responsible for setting/updating the new learning rate (LR) based on the current LR \n",
    "          and also, for recording current loss and LR. \"\"\"\n",
    "        # get the current LR from the model and save to a list of LRs which have been used already \n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "        \n",
    "        # get the loss at the end of this batch, compute the average loss, smooth it and save to a list of losses \n",
    "        l = logs[\"loss\"]\n",
    "        self.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta)*l)\n",
    "        smooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
    "        self.losses.append(smooth)\n",
    "        \n",
    "        # compute maximum loss as a factor of best loss to stop the training \n",
    "        stopLoss = self.stopFactor * self.bestLoss \n",
    "        \n",
    "        # check whether the loss has grown too large, therefore, stop the training\n",
    "        if self.batchNum > 1 and smooth > stopLoss: \n",
    "            self.model.stop_training = True \n",
    "            return \n",
    "        \n",
    "        # check if the best loss should be updated\n",
    "        if self.batchNum == 1 or smooth < self.bestLoss: \n",
    "            self.bestLoss = smooth \n",
    "\n",
    "        # finally increase the LR and set it as new LR for model training \n",
    "        lr *= self.lrMult \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "    def find(self, trainData, startLR, endLR, epochs=None, \n",
    "           stepsPerEpoch=None, batchSize=32, sampleSize=2048, \n",
    "           verbose=1):\n",
    "        \"\"\"finds different learning rates (including optimal one) via model training \n",
    "          and at each batch update, new LR is update and saves current LR and loss. \n",
    "\n",
    "          :param trainData: training data either Numpy array data or data generator \n",
    "          :param startLR: starting learning rate \n",
    "          :param endLR: last learning rate \n",
    "          :param epochs: number of epochs to train for (if not value provided, it is calculate on a default sampleSize)\n",
    "          :param stepsPerEpoch: total number of batch update steps per epoch \n",
    "          :sampleSize: size of training data to use when finding the optimal learning rate \n",
    "        \"\"\"\n",
    "        # reset class specific variables \n",
    "        self.reset()\n",
    "\n",
    "        # determine if we are using a data generator or not \n",
    "        useGen = self.is_data_iter(trainData)\n",
    "\n",
    "        # if we are using a generator and the steps_per_epoch is not supplied, \n",
    "        # raise an error \n",
    "        if useGen and stepsPerEpoch is None: \n",
    "            raise Exception (\"Using generator without supplying steps_per_epoch\")\n",
    "\n",
    "          # if we are not using a generator then our entire dataset must already be \n",
    "          # in memory \n",
    "        elif not useGen:\n",
    "            # get number of samples in the training data and then derive the number of steps_per_epoch \n",
    "            numSamples = len(trainData[0])\n",
    "            stepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
    "\n",
    "            # if number of epochs is not provided, then compute it based on a \n",
    "            # default sample size \n",
    "            if epochs is None:\n",
    "                epochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
    "\n",
    "        # calculate total number of batch updates that will take place \n",
    "        numBatchUpdates = epochs * stepsPerEpoch \n",
    "\n",
    "        # derive the LR multiplier based on ending LR, starting LR and total \n",
    "        # number of batch updates (exponentially increase LR)\n",
    "        self.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
    "\n",
    "        # save the model's original weights, so we can reset the weights when we are \n",
    "        # done finiding the optimal learning rates \n",
    "        self.weightsFile = tempfile.mkstemp()[1] \n",
    "        self.model.save_weights(self.weightsFile)\n",
    "\n",
    "        # save the model's original LR and then set the new \"starting\" learning rate \n",
    "        origLR = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, startLR)\n",
    "\n",
    "        # perform model training and at each batch: update LR and record current LR and loss \n",
    "        # create a callback that will be called at the end of each batch, which increases the LR and \n",
    "        # save current LR and loss\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        # check if we are using a data iterator \n",
    "        if useGen:\n",
    "            self.model.fit_generator(\n",
    "                trainData,\n",
    "                steps_per_epoch=stepsPerEpoch,\n",
    "                epochs=epochs,\n",
    "                verbose=verbose,\n",
    "                callbacks=[callback])\n",
    "\n",
    "        # otherwise, our entire training data is already in memory \n",
    "        else:\n",
    "            self.model.fit(\n",
    "                trainData[0], trainData[1],\n",
    "                batch_size=batchSize,\n",
    "                epochs=epochs,\n",
    "                callbacks=[callback],\n",
    "                verbose=verbose)\n",
    "\n",
    "        # finally, when we are done, set back the original model's weights and LR values \n",
    "        self.model.load_weights(self.weightsFile)\n",
    "        K.set_value(self.model.optimizer.lr, origLR)\n",
    "    \n",
    "    def plot_loss(self, skipBegin=10, skipEnd=1, title=\"learning rate finder\"):\n",
    "        \"\"\" plot learning rates versus losses diagram \"\"\"\n",
    "        lrs = self.lrs[skipBegin:-skipEnd]\n",
    "        losses = self.losses[skipBegin:-skipEnd]\n",
    "\n",
    "        # plot the learning rate versus loss\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 images belonging to 2 classes.\n",
      "Found 430 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data generator and initialize it with data augmentation methods \n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, \n",
    "                                   horizontal_flip=True, \n",
    "                                   vertical_flip=True,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   brightness_range=[0.6, 1.3], \n",
    "                                   zoom_range=0.15, fill_mode=\"reflect\")\n",
    "\n",
    "# and validation data generator\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=12345,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    VALIDATION_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model \n",
    "we are using transfer learning for training our model, therefore, freeze the main layers of the model and attach a new custom F.C. classifier on the top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model \n",
    "def create_model():\n",
    "    base_model = MobileNetV2(include_top=False, \n",
    "                              input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    \n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    input_layer = base_model.output \n",
    "    x = GlobalAveragePooling2D()(input_layer)\n",
    "    x = Dense(128, activation='selu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(NUM_CLASSES, name=\"last_dense\")(x)\n",
    "    output = Activation(\"softmax\", name=\"last_softmax\")(x)\n",
    "    #output = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    # create the final model \n",
    "    model = Model(inputs = base_model.input, outputs = output)\n",
    "    model.summary()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 225, 225, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          163968      global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "last_dense (Dense)              (None, 2)            258         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "last_softmax (Activation)       (None, 2)            0           last_dense[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,422,210\n",
      "Trainable params: 164,226\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "#              metrics=['acc'])\n",
    "\n",
    "#opt = tf.keras.optimizers.SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / EPOCHS)\n",
    "opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "    \"\"\" goal of this class is to provide a plot where effects of using a range \n",
    "    of learning rates on the loss is displayed. Some LRs are too low and some are \n",
    "    too high, therefore, with the help of this plot, we can find an optimal range of \n",
    "    LRs (minimum and maximum bounds). \n",
    "\n",
    "    Starting and ending LRs choosen for the plot are too low (where network is unable \n",
    "    to learn) or too large (where loss is too high). Therefore, a good range of min and \n",
    "    max LR bounds should be somewhere inside of it and that's the aim of this class. \n",
    "\n",
    "    A careful analysis of the plot is required to find the right bounds. At the end, \n",
    "    entire network will be then trained by using correct learning rate (max learning rate) or min and max LRs.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, stopFactor=4, beta=0.98):\n",
    "        \"\"\" initializes variables for finding the learning rates \n",
    "\n",
    "          :param model: model for which learning rates and losses will be analyzed\n",
    "          :param stopFactor: stop factor when the learning rate becomes too large to stop the model training process\n",
    "          :param beta: used for averaging the loss value\n",
    "          :param lrs: a list of tried LR values\n",
    "          :param losses: a list of tried loss values\n",
    "          :param avgLoss: average loss value over time \n",
    "          :param batchNum: current batch number \n",
    "          :param bestLoss: best low (of course lowest) found so far during training\n",
    "          :param lrMult: learning rate multiplication factor \n",
    "          :weightsFile: filename to save initial (original) weights of the model \"\"\"\n",
    "        \n",
    "        self.model = model\n",
    "        self.stopFactor = stopFactor # not clear why 4? \n",
    "\n",
    "        self.beta = beta \n",
    "        self.lrs = [] # list of LRs which have been used already\n",
    "        self.losses = [] # list of losses so far in the batch updates\n",
    "        self.avgLoss = 0\n",
    "        self.batchNum = 0 # current batch number/index\n",
    "\n",
    "        self.bestLoss = 1e9 \n",
    "        self.lrMult = 1\n",
    "        self.weightsFile = None\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" reset or re-initialize all variables from our constructor \"\"\"\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0 \n",
    "        self.bestLoss = 1e9 \n",
    "        self.batchNum = 0 \n",
    "        self.weightsFile = None\n",
    "        \n",
    "    def is_data_iter(self, data):\n",
    "        # define the set of class types we will check for\n",
    "        iterClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\", \n",
    "                     \"DataFrameIterator\", \"Iterator\", \"Sequence\"]\n",
    "        # return whether our data is an iterator\n",
    "        return data.__class__.__name__ in iterClasses\n",
    "    \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        \"\"\" responsible for setting/updating the new learning rate (LR) based on the current LR \n",
    "          and also, for recording current loss and LR. \"\"\"\n",
    "        # get the current LR from the model and save to a list of LRs which have been used already \n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "        \n",
    "        # get the loss at the end of this batch, compute the average loss, smooth it and save to a list of losses \n",
    "        l = logs[\"loss\"]\n",
    "        self.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta)*l)\n",
    "        smooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
    "        self.losses.append(smooth)\n",
    "        \n",
    "        # compute maximum loss as a factor of best loss to stop the training \n",
    "        stopLoss = self.stopFactor * self.bestLoss \n",
    "        \n",
    "        # check whether the loss has grown too large, therefore, stop the training\n",
    "        if self.batchNum > 1 and smooth > stopLoss: \n",
    "            self.model.stop_training = True \n",
    "            return \n",
    "        \n",
    "        # check if the best loss should be updated\n",
    "        if self.batchNum == 1 or smooth < self.bestLoss: \n",
    "            self.bestLoss = smooth \n",
    "\n",
    "        # finally increase the LR and set it as new LR for model training \n",
    "        lr *= self.lrMult \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "    def find(self, trainData, startLR, endLR, epochs=None, \n",
    "           stepsPerEpoch=None, batchSize=32, sampleSize=2048, \n",
    "           verbose=1):\n",
    "        \"\"\"finds different learning rates (including optimal one) via model training \n",
    "          and at each batch update, new LR is update and saves current LR and loss. \n",
    "\n",
    "          :param trainData: training data either Numpy array data or data generator \n",
    "          :param startLR: starting learning rate \n",
    "          :param endLR: last learning rate \n",
    "          :param epochs: number of epochs to train for (if not value provided, it is calculate on a default sampleSize)\n",
    "          :param stepsPerEpoch: total number of batch update steps per epoch \n",
    "          :sampleSize: size of training data to use when finding the optimal learning rate \n",
    "        \"\"\"\n",
    "        # reset class specific variables \n",
    "        self.reset()\n",
    "\n",
    "        # determine if we are using a data generator or not \n",
    "        useGen = self.is_data_iter(trainData)\n",
    "\n",
    "        # if we are using a generator and the steps_per_epoch is not supplied, \n",
    "        # raise an error \n",
    "        if useGen and stepsPerEpoch is None: \n",
    "            raise Exception (\"Using generator without supplying steps_per_epoch\")\n",
    "\n",
    "          # if we are not using a generator then our entire dataset must already be \n",
    "          # in memory \n",
    "        elif not useGen:\n",
    "            # get number of samples in the training data and then derive the number of steps_per_epoch \n",
    "            numSamples = len(trainData[0])\n",
    "            stepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
    "\n",
    "            # if number of epochs is not provided, then compute it based on a \n",
    "            # default sample size \n",
    "            if epochs is None:\n",
    "                epochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
    "\n",
    "        # calculate total number of batch updates that will take place \n",
    "        numBatchUpdates = epochs * stepsPerEpoch \n",
    "\n",
    "        # derive the LR multiplier based on ending LR, starting LR and total \n",
    "        # number of batch updates (exponentially increase LR)\n",
    "        self.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
    "\n",
    "        # save the model's original weights, so we can reset the weights when we are \n",
    "        # done finiding the optimal learning rates \n",
    "        self.weightsFile = tempfile.mkstemp()[1] \n",
    "        self.model.save_weights(self.weightsFile)\n",
    "\n",
    "        # save the model's original LR and then set the new \"starting\" learning rate \n",
    "        origLR = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, startLR)\n",
    "\n",
    "        # perform model training and at each batch: update LR and record current LR and loss \n",
    "        # create a callback that will be called at the end of each batch, which increases the LR and \n",
    "        # save current LR and loss\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        # check if we are using a data iterator \n",
    "        if useGen:\n",
    "            self.model.fit_generator(\n",
    "                trainData,\n",
    "                steps_per_epoch=stepsPerEpoch,\n",
    "                epochs=epochs,\n",
    "                verbose=verbose,\n",
    "                callbacks=[callback])\n",
    "\n",
    "        # otherwise, our entire training data is already in memory \n",
    "        else:\n",
    "            self.model.fit(\n",
    "                trainData[0], trainData[1],\n",
    "                batch_size=batchSize,\n",
    "                epochs=epochs,\n",
    "                callbacks=[callback],\n",
    "                verbose=verbose)\n",
    "\n",
    "        # finally, when we are done, set back the original model's weights and LR values \n",
    "        self.model.load_weights(self.weightsFile)\n",
    "        K.set_value(self.model.optimizer.lr, origLR)\n",
    "    \n",
    "    def plot_loss(self, skipBegin=10, skipEnd=1, title=\"learning rate finder\"):\n",
    "        \"\"\" plot learning rates versus losses diagram \"\"\"\n",
    "        lrs = self.lrs[skipBegin:-skipEnd]\n",
    "        losses = self.losses[skipBegin:-skipEnd]\n",
    "\n",
    "        # plot the learning rate versus loss\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 25 steps\n",
      "Epoch 1/15\n",
      " 1/25 [>.............................] - ETA: 1:43 - loss: 1.0290 - acc: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/ipykernel_launcher.py:68: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/25 [==========================>...] - ETA: 4s - loss: 1.1056 - acc: 0.4980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 60s 2s/step - loss: 1.0969 - acc: 0.4988\n",
      "Epoch 2/15\n",
      "23/25 [==========================>...] - ETA: 4s - loss: 1.0370 - acc: 0.5102"
     ]
    }
   ],
   "source": [
    "lrf = LearningRateFinder(model)\n",
    "lrf.find(\n",
    "    train_generator, \n",
    "    startLR=1e-10, \n",
    "    endLR=1e+1, \n",
    "    epochs=15,\n",
    "    stepsPerEpoch=TRAIN_SAMPLES // BATCH_SIZE, \n",
    "    batchSize=BATCH_SIZE)\n",
    "\n",
    "# plot the loss for the various learning rates and save the\n",
    "# resulting plot to disk\n",
    "lrf.plot_loss()\n",
    "plt.savefig(LR_LOSS_PLOT_PATH)\n",
    "\n",
    "# gracefully exit the script so we can adjust our learning rates\n",
    "# in the config and then train the network for our full set of\n",
    "# epochs\n",
    "print(\"[INFO] learning rate finder complete\")\n",
    "print(\"[INFO] examine plot and adjust learning rates before training\")\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0e71f4f28001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] learning rate finder complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] examine plot and adjust learning rates before training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m: 0"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define callbacks before starting the training\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\", verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(MODEL_PATH, monitor=\"val_acc\", save_best_only=True, mode='max', verbose=1)\n",
    "callbacks = [reduce_lr, early_stop, model_checkpoint]\n",
    "\n",
    "# perform model training \n",
    "H = model.fit_generator(generator=train_generator, \n",
    "                    steps_per_epoch=TRAIN_SAMPLES // BATCH_SIZE, \n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=VALIDATION_SAMPLES // BATCH_SIZE, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results \n",
    "Plot Confusion matrix and classification report on both validation and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mismatches(y_true: list, y_pred: list, filenames: list):\n",
    "    \"\"\" return a list of wrong predictions (or, mis-matches) done by the trained model \n",
    "        :param y_true: a list of true (or ground-truth) labels \n",
    "        :param y_pred: a list of predicted labels \n",
    "        :param filenames: a list of filenames corresponding to the true labels\n",
    "    \"\"\"\n",
    "    mismatches_imgs = list(dict())\n",
    "    for i in range(0, len(y_true)):\n",
    "        if y_true[i] != y_pred[i]:\n",
    "            print(f\"True class: {LABELS[y_true[i]]}({y_true[i]}) and predicted as: {LABELS[y_pred[i]]}({y_pred[i]})\")\n",
    "            temp = dict()\n",
    "            temp[\"filename\"] = filenames[i]\n",
    "            temp[\"true label\"] = y_true[i]\n",
    "            temp[\"pred label\"] = y_pred[i]\n",
    "            mismatches_imgs.append(temp)\n",
    "        continue\n",
    "    return mismatches_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix, classification report and mis-matches information for the validation data\n",
    "y_pred = model.predict_generator(validation_generator, steps=VALIDATION_SAMPLES // BATCH_SIZE + 1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true=validation_generator.classes, y_pred=y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true=validation_generator.classes, y_pred=y_pred, target_names=LABELS))\n",
    "print(\"Mismatches information:\")\n",
    "print(get_mismatches(validation_generator.classes, y_pred, validation_generator.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, plot confusion for the test dataset\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix, classification report and mis-classification (or, mis-matches) information for the test data\n",
    "y_pred = model.predict_generator(test_generator, steps=TEST_SAMPLES // BATCH_SIZE + 1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true=test_generator.classes, y_pred=y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true=test_generator.classes, y_pred=y_pred, target_names=LABELS))\n",
    "print(\"Mismatches information:\")\n",
    "print(get_mismatches(test_generator.classes, y_pred, validation_generator.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
