{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential \n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D, Activation, SeparableConv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import LambdaCallback \n",
    "from tensorflow.keras import backend as K \n",
    "import tempfile \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters \n",
    "TRAIN_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/train/'\n",
    "VALIDATION_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/val/'\n",
    "TEST_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/test/'\n",
    "MODEL_PATH = \"./models/mobilenetv2_.h5\"\n",
    "TRAIN_SAMPLES = 1600\n",
    "VALIDATION_SAMPLES = 430\n",
    "TEST_SAMPLES = 430\n",
    "NUM_CLASSES = 2\n",
    "IMG_WIDTH, IMG_HEIGHT = 128, 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LABELS = [\"fire\", \"nofire\"]\n",
    "LR_LOSS_PLOT_PATH = \"./models/lr_loss_plot.png\"\n",
    "INIT_LR = 1e-2\n",
    "FIND_LR_ENABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data generators\n",
    "create data generators for both training and validation sets and apply data augmentation only to training generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data generator and initialize it with data augmentation methods \n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, \n",
    "                                   horizontal_flip=True, \n",
    "                                   vertical_flip=True,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   brightness_range=[0.6, 1.3], \n",
    "                                   zoom_range=0.15, fill_mode=\"reflect\")\n",
    "\n",
    "# and validation data generator\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=12345,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    VALIDATION_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model \n",
    "we are using transfer learning for training our model, therefore, freeze the main layers of the model and attach a new custom F.C. classifier on the top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireDetectionNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        # CONV => RELU => POOL\n",
    "        model.add(SeparableConv2D(16, (7, 7), padding=\"same\",\n",
    "            input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        # CONV => RELU => POOL\n",
    "        model.add(SeparableConv2D(32, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        # (CONV => RELU) * 2 => POOL\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        # first set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # second set of FC => RELU layers\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] compiling model...\")\n",
    "opt = tf.keras.optimizers.SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / EPOCHS)\n",
    "model = FireDetectionNet.build(width=IMG_WIDTH, height=IMG_HEIGHT, depth=3, classes=2)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "    def __init__(self, model, stopFactor=4, beta=0.98):\n",
    "        # store the model, stop factor, and beta value (for computing\n",
    "        # a smoothed, average loss)\n",
    "        self.model = model\n",
    "        self.stopFactor = stopFactor\n",
    "        self.beta = beta\n",
    "\n",
    "        # initialize our list of learning rates and losses,\n",
    "        # respectively\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "\n",
    "        # initialize our learning rate multiplier, average loss, best\n",
    "        # loss found thus far, current batch number, and weights file\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "\n",
    "    def reset(self):\n",
    "        # re-initialize all variables from our constructor\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "\n",
    "    def is_data_iter(self, data):\n",
    "        # define the set of class types we will check for\n",
    "        iterClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\",\n",
    "             \"DataFrameIterator\", \"Iterator\", \"Sequence\"]\n",
    "\n",
    "        # return whether our data is an iterator\n",
    "        return data.__class__.__name__ in iterClasses\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # grab the current learning rate and add log it to the list of\n",
    "        # learning rates that we've tried\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # grab the loss at the end of this batch, increment the total\n",
    "        # number of batches processed, compute the average average\n",
    "        # loss, smooth it, and update the losses list with the\n",
    "        # smoothed value\n",
    "        l = logs[\"loss\"]\n",
    "        self.batchNum += 1\n",
    "        self.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
    "        smooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
    "        self.losses.append(smooth)\n",
    "\n",
    "        # compute the maximum loss stopping factor value\n",
    "        stopLoss = self.stopFactor * self.bestLoss\n",
    "\n",
    "        # check to see whether the loss has grown too large\n",
    "        if self.batchNum > 1 and smooth > stopLoss:\n",
    "            # stop returning and return from the method\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        # check to see if the best loss should be updated\n",
    "        if self.batchNum == 1 or smooth < self.bestLoss:\n",
    "            self.bestLoss = smooth\n",
    "\n",
    "        # increase the learning rate\n",
    "        lr *= self.lrMult\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def find(self, trainData, startLR, endLR, epochs=None,\n",
    "        stepsPerEpoch=None, batchSize=32, sampleSize=2048,\n",
    "        classWeight=None, verbose=1):\n",
    "        # reset our class-specific variables\n",
    "        self.reset()\n",
    "\n",
    "        # determine if we are using a data generator or not\n",
    "        useGen = self.is_data_iter(trainData)\n",
    "\n",
    "        # if we're using a generator and the steps per epoch is not\n",
    "        # supplied, raise an error\n",
    "        if useGen and stepsPerEpoch is None:\n",
    "            msg = \"Using generator without supplying stepsPerEpoch\"\n",
    "            raise Exception(msg)\n",
    "\n",
    "        # if we're not using a generator then our entire dataset must\n",
    "        # already be in memory\n",
    "        elif not useGen:\n",
    "            # grab the number of samples in the training data and\n",
    "            # then derive the number of steps per epoch\n",
    "            numSamples = len(trainData[0])\n",
    "            stepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
    "\n",
    "        # if no number of training epochs are supplied, compute the\n",
    "        # training epochs based on a default sample size\n",
    "        if epochs is None:\n",
    "            epochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
    "\n",
    "        # compute the total number of batch updates that will take\n",
    "        # place while we are attempting to find a good starting\n",
    "        # learning rate\n",
    "        numBatchUpdates = epochs * stepsPerEpoch\n",
    "\n",
    "        # derive the learning rate multiplier based on the ending\n",
    "        # learning rate, starting learning rate, and total number of\n",
    "        # batch updates\n",
    "        self.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
    "\n",
    "        # create a temporary file path for the model weights and\n",
    "        # then save the weights (so we can reset the weights when we\n",
    "        # are done)\n",
    "        self.weightsFile = tempfile.mkstemp()[1]\n",
    "        self.model.save_weights(self.weightsFile)\n",
    "\n",
    "        # grab the *original* learning rate (so we can reset it\n",
    "        # later), and then set the *starting* learning rate\n",
    "        origLR = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, startLR)\n",
    "\n",
    "        # construct a callback that will be called at the end of each\n",
    "        # batch, enabling us to increase our learning rate as training\n",
    "        # progresses\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs:\n",
    "            self.on_batch_end(batch, logs))\n",
    "\n",
    "        # check to see if we are using a data iterator\n",
    "        if useGen:\n",
    "            self.model.fit_generator(\n",
    "                trainData,\n",
    "                steps_per_epoch=stepsPerEpoch,\n",
    "                epochs=epochs,\n",
    "                class_weight=classWeight,\n",
    "                verbose=verbose,\n",
    "                callbacks=[callback])\n",
    "\n",
    "        # otherwise, our entire training data is already in memory\n",
    "        else:\n",
    "            # train our model using Keras' fit method\n",
    "            self.model.fit(\n",
    "                trainData[0], trainData[1],\n",
    "                batch_size=batchSize,\n",
    "                epochs=epochs,\n",
    "                class_weight=classWeight,\n",
    "                callbacks=[callback],\n",
    "                verbose=verbose)\n",
    "\n",
    "        # restore the original model weights and learning rate\n",
    "        self.model.load_weights(self.weightsFile)\n",
    "        K.set_value(self.model.optimizer.lr, origLR)\n",
    "\n",
    "    def plot_loss(self, skipBegin=10, skipEnd=1, title=\"\"):\n",
    "        # grab the learning rate and losses values to plot\n",
    "        lrs = self.lrs[skipBegin:-skipEnd]\n",
    "        losses = self.losses[skipBegin:-skipEnd]\n",
    "\n",
    "        # plot the learning rate vs. loss\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning Rate (Log Scale)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "\n",
    "        # if the title is not empty, add it to the plot\n",
    "        if title != \"\":\n",
    "            plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIND_LR_ENABLE:\n",
    "    lrf = LearningRateFinder(model)\n",
    "    lrf.find(\n",
    "        train_generator, \n",
    "        startLR=1e-10, \n",
    "        endLR=1e+1, \n",
    "        epochs=20,\n",
    "        stepsPerEpoch=TRAIN_SAMPLES // BATCH_SIZE, \n",
    "        batchSize=BATCH_SIZE)\n",
    "\n",
    "    # plot the loss for the various learning rates and save the\n",
    "    # resulting plot to disk\n",
    "    lrf.plot_loss()\n",
    "    plt.savefig(LR_LOSS_PLOT_PATH)\n",
    "\n",
    "    # gracefully exit the script so we can adjust our learning rates\n",
    "    # in the config and then train the network for our full set of\n",
    "    # epochs\n",
    "    print(\"[INFO] learning rate finder complete\")\n",
    "    print(\"[INFO] examine plot and adjust learning rates before training\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define callbacks before starting the training\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\", verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=3, min_lr=0.00001, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(MODEL_PATH, monitor=\"val_acc\", save_best_only=True, mode='max', verbose=1)\n",
    "callbacks = [reduce_lr, early_stop, model_checkpoint]\n",
    "\n",
    "# perform model training \n",
    "H = model.fit_generator(generator=train_generator, \n",
    "                    steps_per_epoch=TRAIN_SAMPLES // BATCH_SIZE, \n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=VALIDATION_SAMPLES // BATCH_SIZE, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results \n",
    "Plot Confusion matrix and classification report on both validation and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mismatches(y_true: list, y_pred: list, filenames: list):\n",
    "    \"\"\" return a list of wrong predictions (or, mis-matches) done by the trained model \n",
    "        :param y_true: a list of true (or ground-truth) labels \n",
    "        :param y_pred: a list of predicted labels \n",
    "        :param filenames: a list of filenames corresponding to the true labels\n",
    "    \"\"\"\n",
    "    mismatches_imgs = list(dict())\n",
    "    for i in range(0, len(y_true)):\n",
    "        if y_true[i] != y_pred[i]:\n",
    "            print(f\"True class: {LABELS[y_true[i]]}({y_true[i]}) and predicted as: {LABELS[y_pred[i]]}({y_pred[i]})\")\n",
    "            temp = dict()\n",
    "            temp[\"filename\"] = filenames[i]\n",
    "            temp[\"true label\"] = y_true[i]\n",
    "            temp[\"pred label\"] = y_pred[i]\n",
    "            mismatches_imgs.append(temp)\n",
    "        continue\n",
    "    return mismatches_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix, classification report and mis-matches information for the validation data\n",
    "y_pred = model.predict_generator(validation_generator, steps=VALIDATION_SAMPLES // BATCH_SIZE + 1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true=validation_generator.classes, y_pred=y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true=validation_generator.classes, y_pred=y_pred, target_names=LABELS))\n",
    "print(\"Mismatches information:\")\n",
    "print(get_mismatches(validation_generator.classes, y_pred, validation_generator.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, plot confusion for the test dataset\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix, classification report and mis-classification (or, mis-matches) information for the test data\n",
    "y_pred = model.predict_generator(test_generator, steps=TEST_SAMPLES // BATCH_SIZE + 1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true=test_generator.classes, y_pred=y_pred))\n",
    "print(\"-\"*50)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true=test_generator.classes, y_pred=y_pred, target_names=LABELS))\n",
    "print(\"Mismatches information:\")\n",
    "print(get_mismatches(test_generator.classes, y_pred, validation_generator.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
