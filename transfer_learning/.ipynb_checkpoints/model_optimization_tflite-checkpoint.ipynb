{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "Running ML model and making inference on mobile devices or embedded devices comes with certain challenges such as the limited amount of resources such as memory, power and data storage. Therefore it is crucial and critical to deploy only optimized and compressed ML models on devices. \n",
    "\n",
    "Tensorflow Lite (TFLite) provides following strategies for optimizing/quantizing the model: \n",
    "\n",
    "- Post training quantization - quantization after model is trained. \n",
    "- Quantization-aware model training - quantization strategy during model's training. \n",
    "\n",
    "Followings are the various options in Post-training quantization: \n",
    "\n",
    "* No Quantization - convert TF model to TF Lite (.tflite) without any modifications in weights and activations values. This will not have any effect on the model, however, it enables you to load use the model now (.tflite) in your mobile devives. Note: size of this file will going to be a bit less than the original h5 file because of using FlatBuffers. \n",
    "\n",
    "\n",
    "* Quantized model (but only weights) - it is also called a hybrid approach and here we only quantized weights of the trained model. Now, here you can quantize your model either to 16 bit floating point or 8 bit integer from 32 bit floating point. This will compress the model either 2x or 4x times, respectively. However, during the inference time, these 16 bit or 8 bit values will be type-casted again to 32 bits for the computation purposes. Because activation values will be still computed in 32 bit FP. \n",
    "\n",
    "\n",
    "* Full integer quantizations of weights and activations - in addition to weight's quantization, here activation values are also quantized to either 16FP or 8INT. Now for example for 8INT, values will be represented from -128 to 127. Therefore, a scaling is calculated which maps 32FP to 8INT values. These scaling parameters are then found from your representative dataset (ex - validation or test dataset).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import load_img \n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from vis.utils import utils\n",
    "from vis.visualization import visualize_cam, overlay\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters \n",
    "TEST_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/fire_and_smoke_data/test/'\n",
    "MODEL_PATH = \"./models/mobilenetv2.h5\"\n",
    "TFLITE_MODEL_DIR = \"./models/tflite/\"\n",
    "TEST_SAMPLES = 430\n",
    "NUM_CLASSES = 2\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 64\n",
    "LABELS = [\"fire\", \"nofire\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory to save the tflite models if it does not exists\n",
    "if not os.path.exists(TFLITE_MODEL_DIR):\n",
    "    os.makedirs(TFLITE_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Conversion to TFLITE without quantization\n",
    "\n",
    "Reference - Check check to TFLIte model:https://www.tensorflow.org/lite/performance/post_training_integer_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConverterError",
     "evalue": "See console for info.\n2020-02-21 14:59:37.709109: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Selu\n2020-02-21 14:59:37.717640: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 371 operators, 694 arrays (0 quantized)\n2020-02-21 14:59:37.728331: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 371 operators, 694 arrays (0 quantized)\n2020-02-21 14:59:37.748652: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 72 operators, 187 arrays (0 quantized)\n2020-02-21 14:59:37.749781: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 72 operators, 187 arrays (0 quantized)\n2020-02-21 14:59:37.750587: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 72 operators, 187 arrays (0 quantized)\n2020-02-21 14:59:37.751290: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 72 operators, 187 arrays (0 quantized)\n2020-02-21 14:59:37.753117: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 10523008 bytes, theoretical optimal value: 9720192 bytes.\n2020-02-21 14:59:37.753431: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 2371084\n2020-02-21 14:59:37.758308: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MEAN, PAD, SOFTMAX. Here is a list of operators for which you will need custom implementations: Selu.\nTraceback (most recent call last):\n  File \"/anaconda3/envs/tf_venv_2.1/bin/toco_from_protos\", line 8, in <module>\n    sys.exit(main())\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n    _run_main(main, args)\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n    sys.exit(main(argv))\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\n    enable_mlir_converter)\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MEAN, PAD, SOFTMAX. Here is a list of operators for which you will need custom implementations: Selu.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-159f7408f6f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert a tf.Keras model to tflite model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# write the model to a tflite file as binary file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0moutput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         **converter_kwargs)\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_calibration_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[0;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       enable_mlir_converter=enable_mlir_converter)\n\u001b[0m\u001b[1;32m    458\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConverterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"See console for info.\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# Must manually cleanup files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConverterError\u001b[0m: See console for info.\n2020-02-21 14:59:37.709109: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Selu\n2020-02-21 14:59:37.717640: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 371 operators, 694 arrays (0 quantized)\n2020-02-21 14:59:37.728331: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 371 operators, 694 arrays (0 quantized)\n2020-02-21 14:59:37.748652: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 72 operators, 187 arrays (0 quantized)\n2020-02-21 14:59:37.749781: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 72 operators, 187 arrays (0 quantized)\n2020-02-21 14:59:37.750587: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 72 operators, 187 arrays (0 quantized)\n2020-02-21 14:59:37.751290: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 72 operators, 187 arrays (0 quantized)\n2020-02-21 14:59:37.753117: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 10523008 bytes, theoretical optimal value: 9720192 bytes.\n2020-02-21 14:59:37.753431: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 2371084\n2020-02-21 14:59:37.758308: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MEAN, PAD, SOFTMAX. Here is a list of operators for which you will need custom implementations: Selu.\nTraceback (most recent call last):\n  File \"/anaconda3/envs/tf_venv_2.1/bin/toco_from_protos\", line 8, in <module>\n    sys.exit(main())\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/absl/app.py\", line 299, in run\n    _run_main(main, args)\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\n    sys.exit(main(argv))\n  File \"/anaconda3/envs/tf_venv_2.1/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\n    enable_mlir_converter)\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\n and pasting the following:\n\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MEAN, PAD, SOFTMAX. Here is a list of operators for which you will need custom implementations: Selu.\n\n\n"
     ]
    }
   ],
   "source": [
    "# convert a tf.Keras model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_no_quant_file = TFLITE_MODEL_DIR + \"mobilenet_no_quant.tflite\"\n",
    "with open(tflite_no_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Note: there should be hardly reduction in size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conversion to TFLITE with quantization (FP16 and INT8) of weights only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Conversion to INT8 \n",
    "Reference - Check weight quantization section: https://www.tensorflow.org/lite/performance/post_training_quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model with INT8 quantization \n",
    "# Note INT8 quantization is by default! \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_no_quant_file = TFLITE_MODEL_DIR + \"mobilenet_weights_int8_quant.tflite\"\n",
    "with open(tflite_no_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Note: you should see roughly 4x times reduction in the model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Conversion to Float16\n",
    "Reference - https://www.tensorflow.org/lite/performance/post_training_float16_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a tf.Keras model to tflite model with INT8 quantization \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# set optimization to DEFAULT and set float16 as the supported type on the target platform\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] \n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# write the model to a tflite file as binary file\n",
    "tflite_no_quant_file = TFLITE_MODEL_DIR + \"mobilenet_weights_float16_quant.tflite\"\n",
    "with open(tflite_no_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Note: you should see roughly 2x times reduction in the model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conversion to TFLITE with quantization (FP16 and INT8) of weights and activations \n",
    "#### 3.1 Conversion to INT8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
