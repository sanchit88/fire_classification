{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized model benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters \n",
    "TEST_DATA_DIR = '/Users/sanchit/Documents/Projects/Datasets/animals/test/'\n",
    "MODEL_PATH = \"./models/mobilenet.h5\"\n",
    "TFLITE_MODEL_DIR = \"./models/tflite/\"\n",
    "QUANT_TYPE = \"w_fp16\" # no_quant, w_int8 or w_fp16\n",
    "TEST_SAMPLES = 450\n",
    "NUM_CLASSES = 3\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 64\n",
    "LABELS = [\"cats\", \"dogs\", \"panda\"]\n",
    "QUANT_NAME_MAP = {\"no_quant\": \"no quantization\", \"w_int8\": \"weights 8-bit INT quantized\", \n",
    "                  \"w_fp16\": \"weights 16-bit FP quantized\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load quantized model according to the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpreter for weights 16-bit FP quantized loading ...\n"
     ]
    }
   ],
   "source": [
    "if QUANT_TYPE == \"no_quant\":\n",
    "    # model without any quantization\n",
    "    print(f\"interpreter for {QUANT_NAME_MAP[QUANT_TYPE]} loading ...\")\n",
    "    interpret = tf.lite.Interpreter(model_path = TFLITE_MODEL_DIR + \"mobilenet_no_quant.tflite\")\n",
    "    interpret.allocate_tensors() # allocate memory to the model\n",
    "    \n",
    "elif QUANT_TYPE == \"w_int8\":\n",
    "    # model with weights INT8 quantization\n",
    "    print(f\"interpreter for {QUANT_NAME_MAP[QUANT_TYPE]} loading ...\")\n",
    "    interpret = tf.lite.Interpreter(model_path = TFLITE_MODEL_DIR + \"mobilenet_weights_int8_quant.tflite\")\n",
    "    interpret.allocate_tensors() # allocate memory to the model\n",
    "    \n",
    "elif QUANT_TYPE == \"w_fp16\":\n",
    "    # model with weights FP16 quantization \n",
    "    print(f\"interpreter for {QUANT_NAME_MAP[QUANT_TYPE]} loading ...\")\n",
    "    interpret = tf.lite.Interpreter(model_path = TFLITE_MODEL_DIR + \"mobilenet_weights_float16_quant.tflite\")\n",
    "    interpret.allocate_tensors() # allocate memory to the model\n",
    "\n",
    "else:\n",
    "    print(f\"Wrong quantization type has been chosen for {QUANT_NAME_MAP[QUANT_TYPE]}\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of input and output tensors for each model \n",
    "input_ind = interpret.get_input_details()[0][\"index\"]\n",
    "out_ind   = interpret.get_output_details()[0][\"index\"]\n",
    "\n",
    "#input_ind_w_int8 = interpret_weights_int8.get_input_details()[0][\"index\"]\n",
    "#out_ind_w_int8   = interpret_weights_int8.get_output_details()[0][\"index\"]\n",
    "\n",
    "#input_ind_w_fp16 = interpret_weights_fp16.get_input_details()[0][\"index\"]\n",
    "#out_ind_w_fp16   = interpret_weights_fp16.get_output_details()[0][\"index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 450 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References for using interpreter: \n",
    "- (check evaluate the models): https://www.tensorflow.org/lite/performance/post_training_float16_quant \n",
    "- it uses batch generator and batch predictions (TODO later): https://thinkmobile.dev/testing-tensorflow-lite-image-classification-model/ \n",
    "- TinyML book chapter on Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing results for weights 16-bit FP quantized ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save the predicted class label (highest probability one) in a list\n",
    "print(f\"computing results for {QUANT_NAME_MAP[QUANT_TYPE]} ... \\n\")\n",
    "results = []\n",
    "accuracy_count = 0\n",
    "for i in range(TEST_SAMPLES): \n",
    "    \n",
    "    # generate a batch of images \n",
    "    test_image = test_generator.next() \n",
    "    \n",
    "    # set the input image to the input index \n",
    "    interpret.set_tensor(input_ind, test_image[0]) \n",
    "    \n",
    "    # run the inference \n",
    "    interpret.invoke() \n",
    "    \n",
    "    # read the predictions from the output tensor\n",
    "    predictions = interpret.tensor(out_ind) # or, get_tensor(out_ind)\n",
    "    \n",
    "    # get the highest predicted class\n",
    "    pred_class = np.argmax(predictions()[0])\n",
    "    \n",
    "    #print(\"predicted class: \", pred_class, \" and actual class: \", test_generator.classes[i])\n",
    "    \n",
    "    results.append(pred_class)\n",
    "    \n",
    "    if pred_class == test_generator.classes[i]:\n",
    "        accuracy_count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy percentage for weights 16-bit FP quantized: 97.556% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy percentage\n",
    "print(f\"accuracy percentage for {QUANT_NAME_MAP[QUANT_TYPE]}: {round((accuracy_count / TEST_SAMPLES) * 100, 3)}% \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Confusion matrix for weights 16-bit FP quantized: \n",
      "\n",
      "[[146   4   0]\n",
      " [  7 143   0]\n",
      " [  0   0 150]]\n",
      "--------------------------------------------------\n",
      "Classification report for weights 16-bit FP quantized: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.95      0.97      0.96       150\n",
      "        dogs       0.97      0.95      0.96       150\n",
      "       panda       1.00      1.00      1.00       150\n",
      "\n",
      "    accuracy                           0.98       450\n",
      "   macro avg       0.98      0.98      0.98       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Plot confusion matrix, classification report\n",
    "print(\"-\"*50)\n",
    "print(f\"Confusion matrix for {QUANT_NAME_MAP[QUANT_TYPE]}: \\n\")\n",
    "print(confusion_matrix(y_true=test_generator.classes, y_pred=results))\n",
    "print(\"-\"*50)\n",
    "print(f\"Classification report for {QUANT_NAME_MAP[QUANT_TYPE]}: \\n\")\n",
    "print(classification_report(y_true=test_generator.classes, y_pred=results, target_names=LABELS))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
